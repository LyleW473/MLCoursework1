{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wlyle\\Desktop\\DataScienceDataChallenge\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "\n",
    "from src.utils import get_kfold_data, convert_non_numeric_to_numeric, calculate_r2_score, calculate_metrics\n",
    "from src.normalisation import Normaliser\n",
    "from src.constants import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>outcome</th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>...</th>\n",
       "      <th>a6</th>\n",
       "      <th>a7</th>\n",
       "      <th>a8</th>\n",
       "      <th>a9</th>\n",
       "      <th>a10</th>\n",
       "      <th>b6</th>\n",
       "      <th>b7</th>\n",
       "      <th>b8</th>\n",
       "      <th>b9</th>\n",
       "      <th>b10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-26.701232</td>\n",
       "      <td>1.14</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>G</td>\n",
       "      <td>VS1</td>\n",
       "      <td>62.3</td>\n",
       "      <td>56.0</td>\n",
       "      <td>7948</td>\n",
       "      <td>6.73</td>\n",
       "      <td>6.70</td>\n",
       "      <td>...</td>\n",
       "      <td>0.168836</td>\n",
       "      <td>-0.273758</td>\n",
       "      <td>1.107832</td>\n",
       "      <td>1.247795</td>\n",
       "      <td>0.482344</td>\n",
       "      <td>0.489511</td>\n",
       "      <td>-0.321138</td>\n",
       "      <td>0.573382</td>\n",
       "      <td>0.446871</td>\n",
       "      <td>-1.990581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.548093</td>\n",
       "      <td>0.38</td>\n",
       "      <td>Premium</td>\n",
       "      <td>H</td>\n",
       "      <td>VS2</td>\n",
       "      <td>60.5</td>\n",
       "      <td>59.0</td>\n",
       "      <td>898</td>\n",
       "      <td>4.69</td>\n",
       "      <td>4.66</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.256549</td>\n",
       "      <td>0.315373</td>\n",
       "      <td>-0.030326</td>\n",
       "      <td>-0.114335</td>\n",
       "      <td>-1.059588</td>\n",
       "      <td>-1.761360</td>\n",
       "      <td>-1.343951</td>\n",
       "      <td>-1.002550</td>\n",
       "      <td>-0.225030</td>\n",
       "      <td>-0.446653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.612562</td>\n",
       "      <td>0.50</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>E</td>\n",
       "      <td>SI1</td>\n",
       "      <td>60.7</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1351</td>\n",
       "      <td>5.09</td>\n",
       "      <td>5.13</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.193327</td>\n",
       "      <td>-0.657307</td>\n",
       "      <td>-0.591726</td>\n",
       "      <td>-0.446856</td>\n",
       "      <td>-0.765286</td>\n",
       "      <td>-0.816544</td>\n",
       "      <td>-1.397794</td>\n",
       "      <td>-0.477130</td>\n",
       "      <td>0.810509</td>\n",
       "      <td>1.725131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-5.073562</td>\n",
       "      <td>0.70</td>\n",
       "      <td>Premium</td>\n",
       "      <td>D</td>\n",
       "      <td>SI1</td>\n",
       "      <td>61.2</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2512</td>\n",
       "      <td>5.74</td>\n",
       "      <td>5.70</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.740788</td>\n",
       "      <td>-1.778860</td>\n",
       "      <td>-0.825070</td>\n",
       "      <td>0.444932</td>\n",
       "      <td>1.173109</td>\n",
       "      <td>0.453606</td>\n",
       "      <td>-0.263440</td>\n",
       "      <td>0.246210</td>\n",
       "      <td>-0.850503</td>\n",
       "      <td>-0.412950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-14.436557</td>\n",
       "      <td>0.83</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>G</td>\n",
       "      <td>SI2</td>\n",
       "      <td>62.4</td>\n",
       "      <td>54.0</td>\n",
       "      <td>2751</td>\n",
       "      <td>6.01</td>\n",
       "      <td>6.08</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.859322</td>\n",
       "      <td>1.409268</td>\n",
       "      <td>0.861992</td>\n",
       "      <td>1.109063</td>\n",
       "      <td>-1.436722</td>\n",
       "      <td>-1.461618</td>\n",
       "      <td>0.081787</td>\n",
       "      <td>0.258087</td>\n",
       "      <td>0.851146</td>\n",
       "      <td>2.204813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     outcome  carat        cut color clarity  depth  table  price     x     y  \\\n",
       "0 -26.701232   1.14      Ideal     G     VS1   62.3   56.0   7948  6.73  6.70   \n",
       "1   6.548093   0.38    Premium     H     VS2   60.5   59.0    898  4.69  4.66   \n",
       "2   6.612562   0.50  Very Good     E     SI1   60.7   58.0   1351  5.09  5.13   \n",
       "3  -5.073562   0.70    Premium     D     SI1   61.2   58.0   2512  5.74  5.70   \n",
       "4 -14.436557   0.83      Ideal     G     SI2   62.4   54.0   2751  6.01  6.08   \n",
       "\n",
       "   ...        a6        a7        a8        a9       a10        b6        b7  \\\n",
       "0  ...  0.168836 -0.273758  1.107832  1.247795  0.482344  0.489511 -0.321138   \n",
       "1  ... -0.256549  0.315373 -0.030326 -0.114335 -1.059588 -1.761360 -1.343951   \n",
       "2  ... -1.193327 -0.657307 -0.591726 -0.446856 -0.765286 -0.816544 -1.397794   \n",
       "3  ... -1.740788 -1.778860 -0.825070  0.444932  1.173109  0.453606 -0.263440   \n",
       "4  ... -0.859322  1.409268  0.861992  1.109063 -1.436722 -1.461618  0.081787   \n",
       "\n",
       "         b8        b9       b10  \n",
       "0  0.573382  0.446871 -1.990581  \n",
       "1 -1.002550 -0.225030 -0.446653  \n",
       "2 -0.477130  0.810509  1.725131  \n",
       "3  0.246210 -0.850503 -0.412950  \n",
       "4  0.258087  0.851146  2.204813  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['outcome', 'carat', 'cut', 'color', 'clarity', 'depth', 'table', 'price', 'x', 'y', 'z', 'a1', 'a2', 'a3', 'a4', 'a5', 'b1', 'b2', 'b3', 'b4', 'b5', 'a6', 'a7', 'a8', 'a9', 'a10', 'b6', 'b7', 'b8', 'b9', 'b10']\n",
      "['carat', 'depth', 'table', 'price', 'x', 'y', 'z', 'a1', 'a2', 'a3', 'a4', 'a5', 'b1', 'b2', 'b3', 'b4', 'b5', 'a6', 'a7', 'a8', 'a9', 'a10', 'b6', 'b7', 'b8', 'b9', 'b10']\n",
      "['cut', 'color', 'clarity']\n"
     ]
    }
   ],
   "source": [
    "# Find columns\n",
    "all_columns = data.columns.tolist()\n",
    "print(all_columns)\n",
    "\n",
    "numeric_columns = data.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "numeric_columns.remove(\"outcome\") # Remove the target column\n",
    "print(numeric_columns)\n",
    "\n",
    "non_numeric_columns = data.select_dtypes(exclude=[\"number\"]).columns.tolist()\n",
    "print(non_numeric_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cut\n",
      "Ideal        4040\n",
      "Premium      2439\n",
      "Very Good    2296\n",
      "Good          925\n",
      "Fair          300\n",
      "Name: count, dtype: int64\n",
      "color\n",
      "G    2120\n",
      "E    1873\n",
      "F    1746\n",
      "H    1506\n",
      "D    1246\n",
      "I     983\n",
      "J     526\n",
      "Name: count, dtype: int64\n",
      "clarity\n",
      "SI1     2408\n",
      "VS2     2256\n",
      "SI2     1743\n",
      "VS1     1503\n",
      "VVS2     951\n",
      "VVS1     675\n",
      "IF       318\n",
      "I1       146\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for non_numeric_column in non_numeric_columns:\n",
    "    print(data[non_numeric_column].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting non-numeric features to numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['G', 'E', 'F', 'H', 'D', 'I', 'J']\n",
      "        outcome  carat  cut  clarity  depth  table  price     x     y     z  \\\n",
      "0    -26.701232   1.14    0        3   62.3   56.0   7948  6.73  6.70  4.18   \n",
      "1      6.548093   0.38    1        4   60.5   59.0    898  4.69  4.66  2.83   \n",
      "2      6.612562   0.50    2        5   60.7   58.0   1351  5.09  5.13  3.10   \n",
      "3     -5.073562   0.70    1        5   61.2   58.0   2512  5.74  5.70  3.50   \n",
      "4    -14.436557   0.83    0        6   62.4   54.0   2751  6.01  6.08  3.77   \n",
      "...         ...    ...  ...      ...    ...    ...    ...   ...   ...   ...   \n",
      "9995  10.718277   0.33    0        3   62.6   57.0   1002  4.42  4.40  2.76   \n",
      "9996 -12.246698   1.01    4        5   69.5   55.0   4853  6.00  5.94  4.15   \n",
      "9997  11.122516   0.52    2        6   57.9   61.0   1273  5.28  5.33  3.07   \n",
      "9998 -24.730782   0.31    0        0   62.0   54.0    801  4.35  4.39  2.71   \n",
      "9999   8.735755   0.37    2        5   59.9   59.0    649  4.68  4.70  2.81   \n",
      "\n",
      "      ...        b8        b9       b10  colour_G  colour_E  colour_F  \\\n",
      "0     ...  0.573382  0.446871 -1.990581         1         0         0   \n",
      "1     ... -1.002550 -0.225030 -0.446653         0         0         0   \n",
      "2     ... -0.477130  0.810509  1.725131         0         1         0   \n",
      "3     ...  0.246210 -0.850503 -0.412950         0         0         0   \n",
      "4     ...  0.258087  0.851146  2.204813         1         0         0   \n",
      "...   ...       ...       ...       ...       ...       ...       ...   \n",
      "9995  ... -1.981229 -0.805800  1.051560         0         1         0   \n",
      "9996  ...  0.114660  0.856687  0.923238         0         1         0   \n",
      "9997  ...  1.022618  1.193452 -0.035714         0         0         1   \n",
      "9998  ... -0.713252  0.133960 -1.547468         1         0         0   \n",
      "9999  ... -0.201825 -0.484968  0.065408         0         1         0   \n",
      "\n",
      "      colour_H  colour_D  colour_I  colour_J  \n",
      "0            0         0         0         0  \n",
      "1            1         0         0         0  \n",
      "2            0         0         0         0  \n",
      "3            0         1         0         0  \n",
      "4            0         0         0         0  \n",
      "...        ...       ...       ...       ...  \n",
      "9995         0         0         0         0  \n",
      "9996         0         0         0         0  \n",
      "9997         0         0         0         0  \n",
      "9998         0         0         0         0  \n",
      "9999         0         0         0         0  \n",
      "\n",
      "[10000 rows x 37 columns]\n",
      "        outcome  carat  cut  clarity  depth  table  price     x     y     z  \\\n",
      "0    -26.701232   1.14    0        3   62.3   56.0   7948  6.73  6.70  4.18   \n",
      "1      6.548093   0.38    1        4   60.5   59.0    898  4.69  4.66  2.83   \n",
      "2      6.612562   0.50    2        5   60.7   58.0   1351  5.09  5.13  3.10   \n",
      "3     -5.073562   0.70    1        5   61.2   58.0   2512  5.74  5.70  3.50   \n",
      "4    -14.436557   0.83    0        6   62.4   54.0   2751  6.01  6.08  3.77   \n",
      "...         ...    ...  ...      ...    ...    ...    ...   ...   ...   ...   \n",
      "9995  10.718277   0.33    0        3   62.6   57.0   1002  4.42  4.40  2.76   \n",
      "9996 -12.246698   1.01    4        5   69.5   55.0   4853  6.00  5.94  4.15   \n",
      "9997  11.122516   0.52    2        6   57.9   61.0   1273  5.28  5.33  3.07   \n",
      "9998 -24.730782   0.31    0        0   62.0   54.0    801  4.35  4.39  2.71   \n",
      "9999   8.735755   0.37    2        5   59.9   59.0    649  4.68  4.70  2.81   \n",
      "\n",
      "      ...        b8        b9       b10  colour_G  colour_E  colour_F  \\\n",
      "0     ...  0.573382  0.446871 -1.990581         1         0         0   \n",
      "1     ... -1.002550 -0.225030 -0.446653         0         0         0   \n",
      "2     ... -0.477130  0.810509  1.725131         0         1         0   \n",
      "3     ...  0.246210 -0.850503 -0.412950         0         0         0   \n",
      "4     ...  0.258087  0.851146  2.204813         1         0         0   \n",
      "...   ...       ...       ...       ...       ...       ...       ...   \n",
      "9995  ... -1.981229 -0.805800  1.051560         0         1         0   \n",
      "9996  ...  0.114660  0.856687  0.923238         0         1         0   \n",
      "9997  ...  1.022618  1.193452 -0.035714         0         0         1   \n",
      "9998  ... -0.713252  0.133960 -1.547468         1         0         0   \n",
      "9999  ... -0.201825 -0.484968  0.065408         0         1         0   \n",
      "\n",
      "      colour_H  colour_D  colour_I  colour_J  \n",
      "0            0         0         0         0  \n",
      "1            1         0         0         0  \n",
      "2            0         0         0         0  \n",
      "3            0         1         0         0  \n",
      "4            0         0         0         0  \n",
      "...        ...       ...       ...       ...  \n",
      "9995         0         0         0         0  \n",
      "9996         0         0         0         0  \n",
      "9997         0         0         0         0  \n",
      "9998         0         0         0         0  \n",
      "9999         0         0         0         0  \n",
      "\n",
      "[10000 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "data = convert_non_numeric_to_numeric(data=data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalise data using each columns respective mean and std."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        outcome  carat  cut  clarity  depth  table  price     x     y     z  \\\n",
      "0    -26.701232   1.14    0        3   62.3   56.0   7948  6.73  6.70  4.18   \n",
      "1      6.548093   0.38    1        4   60.5   59.0    898  4.69  4.66  2.83   \n",
      "2      6.612562   0.50    2        5   60.7   58.0   1351  5.09  5.13  3.10   \n",
      "3     -5.073562   0.70    1        5   61.2   58.0   2512  5.74  5.70  3.50   \n",
      "4    -14.436557   0.83    0        6   62.4   54.0   2751  6.01  6.08  3.77   \n",
      "...         ...    ...  ...      ...    ...    ...    ...   ...   ...   ...   \n",
      "9995  10.718277   0.33    0        3   62.6   57.0   1002  4.42  4.40  2.76   \n",
      "9996 -12.246698   1.01    4        5   69.5   55.0   4853  6.00  5.94  4.15   \n",
      "9997  11.122516   0.52    2        6   57.9   61.0   1273  5.28  5.33  3.07   \n",
      "9998 -24.730782   0.31    0        0   62.0   54.0    801  4.35  4.39  2.71   \n",
      "9999   8.735755   0.37    2        5   59.9   59.0    649  4.68  4.70  2.81   \n",
      "\n",
      "      ...        b8        b9       b10  colour_G  colour_E  colour_F  \\\n",
      "0     ...  0.573382  0.446871 -1.990581         1         0         0   \n",
      "1     ... -1.002550 -0.225030 -0.446653         0         0         0   \n",
      "2     ... -0.477130  0.810509  1.725131         0         1         0   \n",
      "3     ...  0.246210 -0.850503 -0.412950         0         0         0   \n",
      "4     ...  0.258087  0.851146  2.204813         1         0         0   \n",
      "...   ...       ...       ...       ...       ...       ...       ...   \n",
      "9995  ... -1.981229 -0.805800  1.051560         0         1         0   \n",
      "9996  ...  0.114660  0.856687  0.923238         0         1         0   \n",
      "9997  ...  1.022618  1.193452 -0.035714         0         0         1   \n",
      "9998  ... -0.713252  0.133960 -1.547468         1         0         0   \n",
      "9999  ... -0.201825 -0.484968  0.065408         0         1         0   \n",
      "\n",
      "      colour_H  colour_D  colour_I  colour_J  \n",
      "0            0         0         0         0  \n",
      "1            1         0         0         0  \n",
      "2            0         0         0         0  \n",
      "3            0         1         0         0  \n",
      "4            0         0         0         0  \n",
      "...        ...       ...       ...       ...  \n",
      "9995         0         0         0         0  \n",
      "9996         0         0         0         0  \n",
      "9997         0         0         0         0  \n",
      "9998         0         0         0         0  \n",
      "9999         0         0         0         0  \n",
      "\n",
      "[10000 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       1.14\n",
      "1       0.38\n",
      "2       0.50\n",
      "3       0.70\n",
      "4       0.83\n",
      "        ... \n",
      "9995    0.33\n",
      "9996    1.01\n",
      "9997    0.52\n",
      "9998    0.31\n",
      "9999    0.37\n",
      "Name: carat, Length: 10000, dtype: float64\n",
      "after 0       0.723643\n",
      "1      -0.886369\n",
      "2      -0.632156\n",
      "3      -0.208469\n",
      "4       0.066928\n",
      "          ...   \n",
      "9995   -0.992290\n",
      "9996    0.448246\n",
      "9997   -0.589788\n",
      "9998   -1.034659\n",
      "9999   -0.907553\n",
      "Name: carat, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "normaliser = Normaliser()\n",
    "for column in numeric_columns:\n",
    "    print(data[column])\n",
    "    data[column] = normaliser.standardise(data[column])\n",
    "    print(\"after\", data[column])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        outcome     carat  cut  clarity  depth  table  price     x     y  \\\n",
      "0    -26.701232  0.723643    0        3   62.3   56.0   7948  6.73  6.70   \n",
      "1      6.548093 -0.886369    1        4   60.5   59.0    898  4.69  4.66   \n",
      "2      6.612562 -0.632156    2        5   60.7   58.0   1351  5.09  5.13   \n",
      "3     -5.073562 -0.208469    1        5   61.2   58.0   2512  5.74  5.70   \n",
      "4    -14.436557  0.066928    0        6   62.4   54.0   2751  6.01  6.08   \n",
      "...         ...       ...  ...      ...    ...    ...    ...   ...   ...   \n",
      "9995  10.718277 -0.992290    0        3   62.6   57.0   1002  4.42  4.40   \n",
      "9996 -12.246698  0.448246    4        5   69.5   55.0   4853  6.00  5.94   \n",
      "9997  11.122516 -0.589788    2        6   57.9   61.0   1273  5.28  5.33   \n",
      "9998 -24.730782 -1.034659    0        0   62.0   54.0    801  4.35  4.39   \n",
      "9999   8.735755 -0.907553    2        5   59.9   59.0    649  4.68  4.70   \n",
      "\n",
      "         z  ...        b8        b9       b10  colour_G  colour_E  colour_F  \\\n",
      "0     4.18  ...  0.573382  0.446871 -1.990581         1         0         0   \n",
      "1     2.83  ... -1.002550 -0.225030 -0.446653         0         0         0   \n",
      "2     3.10  ... -0.477130  0.810509  1.725131         0         1         0   \n",
      "3     3.50  ...  0.246210 -0.850503 -0.412950         0         0         0   \n",
      "4     3.77  ...  0.258087  0.851146  2.204813         1         0         0   \n",
      "...    ...  ...       ...       ...       ...       ...       ...       ...   \n",
      "9995  2.76  ... -1.981229 -0.805800  1.051560         0         1         0   \n",
      "9996  4.15  ...  0.114660  0.856687  0.923238         0         1         0   \n",
      "9997  3.07  ...  1.022618  1.193452 -0.035714         0         0         1   \n",
      "9998  2.71  ... -0.713252  0.133960 -1.547468         1         0         0   \n",
      "9999  2.81  ... -0.201825 -0.484968  0.065408         0         1         0   \n",
      "\n",
      "      colour_H  colour_D  colour_I  colour_J  \n",
      "0            0         0         0         0  \n",
      "1            1         0         0         0  \n",
      "2            0         0         0         0  \n",
      "3            0         1         0         0  \n",
      "4            0         0         0         0  \n",
      "...        ...       ...       ...       ...  \n",
      "9995         0         0         0         0  \n",
      "9996         0         0         0         0  \n",
      "9997         0         0         0         0  \n",
      "9998         0         0         0         0  \n",
      "9999         0         0         0         0  \n",
      "\n",
      "[10000 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0/5\n",
      "Train shape: (6400, 37) | 64.00%\n",
      "Validation shape: (1600, 37) | 16.00%\n",
      "Test shape: (2000, 37) | 20.00%\n",
      "\n",
      "Fold: 1/5\n",
      "Train shape: (6400, 37) | 64.00%\n",
      "Validation shape: (1600, 37) | 16.00%\n",
      "Test shape: (2000, 37) | 20.00%\n",
      "\n",
      "Fold: 2/5\n",
      "Train shape: (6400, 37) | 64.00%\n",
      "Validation shape: (1600, 37) | 16.00%\n",
      "Test shape: (2000, 37) | 20.00%\n",
      "\n",
      "Fold: 3/5\n",
      "Train shape: (6400, 37) | 64.00%\n",
      "Validation shape: (1600, 37) | 16.00%\n",
      "Test shape: (2000, 37) | 20.00%\n",
      "\n",
      "Fold: 4/5\n",
      "Train shape: (6400, 37) | 64.00%\n",
      "Validation shape: (1600, 37) | 16.00%\n",
      "Test shape: (2000, 37) | 20.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kfold_data = get_kfold_data(data=data, k=NUM_FOLDS, reproducibility_seed=REPRODUCIBILITY_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define models and hyperparameter tuning objectives for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(model_type, trial, x_train, y_train, x_val, y_val):\n",
    "    if model_type == lgb.LGBMRegressor:\n",
    "        parameters = {\n",
    "                    \"objective\": \"regression\",\n",
    "                    \"metric\": \"rmse\",\n",
    "                    \"n_estimators\": 100,\n",
    "                    \"verbosity\": -1,\n",
    "                    \"bagging_freq\": 1,\n",
    "                    \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.1, log=True),\n",
    "                    \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 2**10),\n",
    "                    \"subsample\": trial.suggest_float(\"subsample\", 0.05, 1.0),\n",
    "                    \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.05, 1.0),\n",
    "                    \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 1, 100),\n",
    "                    \"seed\": REPRODUCIBILITY_SEED\n",
    "        }\n",
    "    elif model_type == xgb.XGBRegressor:\n",
    "        parameters = {\n",
    "            \"objective\": \"reg:squarederror\",\n",
    "            \"eval_metric\": \"rmse\",\n",
    "            \"n_estimators\": 100,\n",
    "            \"eta\": trial.suggest_float(\"eta\", 1e-2, 0.2, log=True),\n",
    "            \"gamma\": trial.suggest_float(\"gamma\", 1e-8, 10, log=True),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "            \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 6),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "            \"seed\": REPRODUCIBILITY_SEED\n",
    "        }\n",
    "    elif model_type == RandomForestRegressor:\n",
    "        parameters = {\n",
    "            \"n_estimators\": 100,\n",
    "            \"criterion\": trial.suggest_categorical(\"criterion\", [\"absolute_error\", \"squared_error\"]),\n",
    "            \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\"]),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),\n",
    "            \"bootstrap\": True,\n",
    "            \"oob_score\": False,\n",
    "            \"n_jobs\": -1,\n",
    "            \"random_state\": REPRODUCIBILITY_SEED\n",
    "        }\n",
    "    elif model_type == GradientBoostingRegressor:\n",
    "        parameters = {\n",
    "            \"n_estimators\": 100,\n",
    "            \"loss\": trial.suggest_categorical(\"loss\", [\"absolute_error\", \"squared_error\", \"huber\", \"quantile\"]),\n",
    "            \"criterion\": trial.suggest_categorical(\"criterion\", [\"friedman_mse\", \"squared_error\"]),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.1, log=True),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.05, 1.0),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),\n",
    "            \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\"]),\n",
    "            \"max_leaf_nodes\": trial.suggest_int(\"max_leaf_nodes\", 2, 2**10),\n",
    "            \"random_state\": REPRODUCIBILITY_SEED\n",
    "        }\n",
    "    elif model_type == AdaBoostRegressor:\n",
    "        parameters = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 100),\n",
    "            \"loss\": trial.suggest_categorical(\"loss\", [\"linear\", \"square\", \"exponential\"]),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.1, log=True),\n",
    "            \"random_state\": REPRODUCIBILITY_SEED\n",
    "        }\n",
    "    elif model_type == Ridge:\n",
    "        parameters = {\n",
    "            \"alpha\": trial.suggest_float(\"alpha\", 1e-3, 0.1, log=True),\n",
    "            \"solver\": trial.suggest_categorical(\"solver\", [\"auto\", \"svd\", \"cholesky\", \"lsqr\", \"sparse_cg\", \"sag\", \"saga\"]),\n",
    "            \"fit_intercept\": trial.suggest_categorical(\"fit_intercept\", [True, False]),\n",
    "            \"positive\": False,\n",
    "            \"random_state\": REPRODUCIBILITY_SEED\n",
    "        }\n",
    "    elif model_type == Lasso:\n",
    "        parameters = {\n",
    "            \"alpha\": trial.suggest_float(\"alpha\", 1e-3, 0.1, log=True),\n",
    "            \"fit_intercept\": trial.suggest_categorical(\"fit_intercept\", [True, False]),\n",
    "            \"selection\": trial.suggest_categorical(\"selection\", [\"cyclic\", \"random\"]),\n",
    "            \"warm_start\": trial.suggest_categorical(\"warm_start\", [True, False]),\n",
    "            \"random_state\": REPRODUCIBILITY_SEED\n",
    "        }\n",
    "    elif model_type == LinearRegression:\n",
    "        parameters = {\n",
    "            \"fit_intercept\": trial.suggest_categorical(\"fit_intercept\", [True, False]),\n",
    "        }\n",
    "    \n",
    "    model = model_type(**parameters) # Create the model\n",
    "    model.fit(x_train, y_train)\n",
    "    predictions = model.predict(x_val)\n",
    "    metrics = calculate_metrics(targets=y_val, preds=predictions)\n",
    "    rmse = metrics[\"rmse\"]\n",
    "    return rmse\n",
    "\n",
    "\n",
    "models = {\n",
    "        # \"linear_regression\": LinearRegression,\n",
    "        # \"lasso\": Lasso,\n",
    "        \"ridge\": Ridge,\n",
    "        # \"xgb\": xgb.XGBRegressor,\n",
    "        # \"random_forest\": RandomForestRegressor,\n",
    "        # \"gradient_boosting\": GradientBoostingRegressor,\n",
    "        # \"ada_boost\": AdaBoostRegressor,\n",
    "        # \"lgbm\": lgb.LGBMRegressor\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-16 15:44:05,279] A new study created in memory with name: no-name-71b3882d-f9fa-42b3-b884-342aa40bcf20\n",
      "c:\\Users\\wlyle\\Desktop\\DataScienceDataChallenge\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-02-16 15:44:06,277] Trial 0 finished with value: 12.948190315309555 and parameters: {'alpha': 0.004648079654055129, 'solver': 'saga', 'fit_intercept': True}. Best is trial 0 with value: 12.948190315309555.\n",
      "[I 2025-02-16 15:44:06,287] Trial 1 finished with value: 10.946969721581594 and parameters: {'alpha': 0.04680742735113145, 'solver': 'svd', 'fit_intercept': False}. Best is trial 1 with value: 10.946969721581594.\n",
      "[I 2025-02-16 15:44:06,293] Trial 2 finished with value: 10.916909495021754 and parameters: {'alpha': 0.004106965439608222, 'solver': 'sparse_cg', 'fit_intercept': True}. Best is trial 2 with value: 10.916909495021754.\n",
      "[I 2025-02-16 15:44:06,298] Trial 3 finished with value: 10.916909558377057 and parameters: {'alpha': 0.0054274441075457544, 'solver': 'sparse_cg', 'fit_intercept': True}. Best is trial 2 with value: 10.916909495021754.\n",
      "[I 2025-02-16 15:44:06,303] Trial 4 finished with value: 10.916908537361497 and parameters: {'alpha': 0.027615436701966603, 'solver': 'sparse_cg', 'fit_intercept': True}. Best is trial 4 with value: 10.916908537361497.\n",
      "[I 2025-02-16 15:44:06,311] Trial 5 finished with value: 10.92057222187152 and parameters: {'alpha': 0.013756517153469228, 'solver': 'svd', 'fit_intercept': False}. Best is trial 4 with value: 10.916908537361497.\n",
      "[I 2025-02-16 15:44:06,318] Trial 6 finished with value: 10.916908784667188 and parameters: {'alpha': 0.019196897928483277, 'solver': 'sparse_cg', 'fit_intercept': True}. Best is trial 4 with value: 10.916908537361497.\n",
      "[I 2025-02-16 15:44:06,323] Trial 7 finished with value: 10.916909359185485 and parameters: {'alpha': 0.005326923006781526, 'solver': 'sparse_cg', 'fit_intercept': True}. Best is trial 4 with value: 10.916908537361497.\n",
      "[I 2025-02-16 15:44:06,328] Trial 8 finished with value: 10.91536515593749 and parameters: {'alpha': 0.004066431606278518, 'solver': 'cholesky', 'fit_intercept': False}. Best is trial 8 with value: 10.91536515593749.\n",
      "c:\\Users\\wlyle\\Desktop\\DataScienceDataChallenge\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-02-16 15:44:07,244] Trial 9 finished with value: 12.926647642669966 and parameters: {'alpha': 0.0443089593020501, 'solver': 'sag', 'fit_intercept': False}. Best is trial 8 with value: 10.91536515593749.\n",
      "[I 2025-02-16 15:44:07,250] Trial 10 finished with value: 10.914499774523883 and parameters: {'alpha': 0.0017156252036040107, 'solver': 'cholesky', 'fit_intercept': False}. Best is trial 10 with value: 10.914499774523883.\n",
      "[I 2025-02-16 15:44:07,256] Trial 11 finished with value: 10.914319626275107 and parameters: {'alpha': 0.001145657817596892, 'solver': 'cholesky', 'fit_intercept': False}. Best is trial 11 with value: 10.914319626275107.\n",
      "[I 2025-02-16 15:44:07,263] Trial 12 finished with value: 10.914299390599686 and parameters: {'alpha': 0.0010791048418565432, 'solver': 'cholesky', 'fit_intercept': False}. Best is trial 12 with value: 10.914299390599686.\n",
      "[I 2025-02-16 15:44:07,269] Trial 13 finished with value: 12.36509821685061 and parameters: {'alpha': 0.0010737675795698671, 'solver': 'lsqr', 'fit_intercept': False}. Best is trial 12 with value: 10.914299390599686.\n",
      "[I 2025-02-16 15:44:07,275] Trial 14 finished with value: 10.914494716321181 and parameters: {'alpha': 0.0017001286243369434, 'solver': 'auto', 'fit_intercept': False}. Best is trial 12 with value: 10.914299390599686.\n",
      "[I 2025-02-16 15:44:07,282] Trial 15 finished with value: 10.985171381778736 and parameters: {'alpha': 0.09388987192892827, 'solver': 'cholesky', 'fit_intercept': False}. Best is trial 12 with value: 10.914299390599686.\n",
      "[I 2025-02-16 15:44:07,288] Trial 16 finished with value: 10.914636053715766 and parameters: {'alpha': 0.0021234590586831623, 'solver': 'cholesky', 'fit_intercept': False}. Best is trial 12 with value: 10.914299390599686.\n",
      "[I 2025-02-16 15:44:07,294] Trial 17 finished with value: 10.914283593305631 and parameters: {'alpha': 0.001026757682656902, 'solver': 'cholesky', 'fit_intercept': False}. Best is trial 17 with value: 10.914283593305631.\n",
      "[I 2025-02-16 15:44:07,299] Trial 18 finished with value: 10.914941460639149 and parameters: {'alpha': 0.002979620926911448, 'solver': 'auto', 'fit_intercept': False}. Best is trial 17 with value: 10.914283593305631.\n",
      "[I 2025-02-16 15:44:07,305] Trial 19 finished with value: 12.365098232286318 and parameters: {'alpha': 0.010057279743983917, 'solver': 'lsqr', 'fit_intercept': False}. Best is trial 17 with value: 10.914283593305631.\n",
      "c:\\Users\\wlyle\\Desktop\\DataScienceDataChallenge\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-02-16 15:44:08,205] Trial 20 finished with value: 12.926647637603152 and parameters: {'alpha': 0.002314779424525703, 'solver': 'sag', 'fit_intercept': False}. Best is trial 17 with value: 10.914283593305631.\n",
      "[I 2025-02-16 15:44:08,212] Trial 21 finished with value: 10.914280123207732 and parameters: {'alpha': 0.001015212017870282, 'solver': 'cholesky', 'fit_intercept': False}. Best is trial 21 with value: 10.914280123207732.\n",
      "[I 2025-02-16 15:44:08,219] Trial 22 finished with value: 10.9142777458345 and parameters: {'alpha': 0.0010072922031343196, 'solver': 'cholesky', 'fit_intercept': False}. Best is trial 22 with value: 10.9142777458345.\n",
      "[I 2025-02-16 15:44:08,227] Trial 23 finished with value: 10.914286005936884 and parameters: {'alpha': 0.0010347749425017093, 'solver': 'cholesky', 'fit_intercept': False}. Best is trial 22 with value: 10.9142777458345.\n",
      "c:\\Users\\wlyle\\Desktop\\DataScienceDataChallenge\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-02-16 15:44:09,239] Trial 24 finished with value: 12.967899663483836 and parameters: {'alpha': 0.0015670452325555293, 'solver': 'saga', 'fit_intercept': False}. Best is trial 22 with value: 10.9142777458345.\n",
      "[I 2025-02-16 15:44:09,247] Trial 25 finished with value: 10.9149336772037 and parameters: {'alpha': 0.0029586564549736825, 'solver': 'cholesky', 'fit_intercept': False}. Best is trial 22 with value: 10.9142777458345.\n",
      "[I 2025-02-16 15:44:09,253] Trial 26 finished with value: 10.914448790118609 and parameters: {'alpha': 0.001558159665470577, 'solver': 'cholesky', 'fit_intercept': False}. Best is trial 22 with value: 10.9142777458345.\n",
      "[I 2025-02-16 15:44:09,259] Trial 27 finished with value: 10.916918216660045 and parameters: {'alpha': 0.007452511654227881, 'solver': 'cholesky', 'fit_intercept': False}. Best is trial 22 with value: 10.9142777458345.\n",
      "[I 2025-02-16 15:44:09,266] Trial 28 finished with value: 10.914836749499752 and parameters: {'alpha': 0.0026940783624783606, 'solver': 'cholesky', 'fit_intercept': False}. Best is trial 22 with value: 10.9142777458345.\n",
      "c:\\Users\\wlyle\\Desktop\\DataScienceDataChallenge\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-02-16 15:44:10,265] Trial 29 finished with value: 12.948190315032592 and parameters: {'alpha': 0.0015920823758611617, 'solver': 'saga', 'fit_intercept': True}. Best is trial 22 with value: 10.9142777458345.\n",
      "[I 2025-02-16 15:44:10,269] A new study created in memory with name: no-name-b901cc65-9237-455c-81f2-5fee0e38c287\n",
      "[I 2025-02-16 15:44:10,274] Trial 0 finished with value: 11.99372325761868 and parameters: {'alpha': 0.046759872144473205, 'solver': 'sparse_cg', 'fit_intercept': False}. Best is trial 0 with value: 11.99372325761868.\n",
      "[I 2025-02-16 15:44:10,279] Trial 1 finished with value: 10.758844617004273 and parameters: {'alpha': 0.045089441417665715, 'solver': 'sparse_cg', 'fit_intercept': True}. Best is trial 1 with value: 10.758844617004273.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1/5\n",
      "Model name: ridge\n",
      "MAE: 8.776012714253206\n",
      "MSE: 119.12145871321823\n",
      "RMSE: 10.9142777458345\n",
      "PCC: 0.5462868819947289\n",
      "Spearman R: 0.5695827010088677\n",
      "R2 Score: 0.297964776661807\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wlyle\\Desktop\\DataScienceDataChallenge\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-02-16 15:44:11,298] Trial 2 finished with value: 12.565087906251122 and parameters: {'alpha': 0.05517392030235027, 'solver': 'saga', 'fit_intercept': False}. Best is trial 1 with value: 10.758844617004273.\n",
      "[I 2025-02-16 15:44:11,306] Trial 3 finished with value: 10.829292088556562 and parameters: {'alpha': 0.06252286819365524, 'solver': 'svd', 'fit_intercept': False}. Best is trial 1 with value: 10.758844617004273.\n",
      "[I 2025-02-16 15:44:11,310] Trial 4 finished with value: 11.799601032710045 and parameters: {'alpha': 0.023385809396966405, 'solver': 'lsqr', 'fit_intercept': True}. Best is trial 1 with value: 10.758844617004273.\n",
      "c:\\Users\\wlyle\\Desktop\\DataScienceDataChallenge\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-02-16 15:44:12,291] Trial 5 finished with value: 12.549082601478197 and parameters: {'alpha': 0.0019421041433268795, 'solver': 'saga', 'fit_intercept': True}. Best is trial 1 with value: 10.758844617004273.\n",
      "c:\\Users\\wlyle\\Desktop\\DataScienceDataChallenge\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-02-16 15:44:13,258] Trial 6 finished with value: 12.549082603180455 and parameters: {'alpha': 0.02296491279364727, 'solver': 'saga', 'fit_intercept': True}. Best is trial 1 with value: 10.758844617004273.\n",
      "[I 2025-02-16 15:44:13,261] Trial 7 finished with value: 10.817871783821674 and parameters: {'alpha': 0.05309681989460033, 'solver': 'cholesky', 'fit_intercept': False}. Best is trial 1 with value: 10.758844617004273.\n",
      "[I 2025-02-16 15:44:13,266] Trial 8 finished with value: 10.754782377269645 and parameters: {'alpha': 0.0014456362599503743, 'solver': 'auto', 'fit_intercept': True}. Best is trial 8 with value: 10.754782377269645.\n",
      "[I 2025-02-16 15:44:13,270] Trial 9 finished with value: 11.993724350760248 and parameters: {'alpha': 0.0022568221743487844, 'solver': 'sparse_cg', 'fit_intercept': False}. Best is trial 8 with value: 10.754782377269645.\n",
      "[I 2025-02-16 15:44:13,276] Trial 10 finished with value: 10.754780794096229 and parameters: {'alpha': 0.00579517061812616, 'solver': 'auto', 'fit_intercept': True}. Best is trial 10 with value: 10.754780794096229.\n",
      "[I 2025-02-16 15:44:13,281] Trial 11 finished with value: 10.754780918737945 and parameters: {'alpha': 0.005452675068961502, 'solver': 'auto', 'fit_intercept': True}. Best is trial 10 with value: 10.754780794096229.\n",
      "[I 2025-02-16 15:44:13,288] Trial 12 finished with value: 10.754780969016014 and parameters: {'alpha': 0.005314521900315189, 'solver': 'auto', 'fit_intercept': True}. Best is trial 10 with value: 10.754780794096229.\n",
      "c:\\Users\\wlyle\\Desktop\\DataScienceDataChallenge\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-02-16 15:44:14,181] Trial 13 finished with value: 12.48190776035904 and parameters: {'alpha': 0.0053887635804027445, 'solver': 'sag', 'fit_intercept': True}. Best is trial 10 with value: 10.754780794096229.\n",
      "[I 2025-02-16 15:44:14,187] Trial 14 finished with value: 10.754780321756705 and parameters: {'alpha': 0.0070931787512674965, 'solver': 'auto', 'fit_intercept': True}. Best is trial 14 with value: 10.754780321756705.\n",
      "[I 2025-02-16 15:44:14,193] Trial 15 finished with value: 10.754778805240587 and parameters: {'alpha': 0.011261637221381189, 'solver': 'auto', 'fit_intercept': True}. Best is trial 15 with value: 10.754778805240587.\n",
      "[I 2025-02-16 15:44:14,199] Trial 16 finished with value: 10.754778731282741 and parameters: {'alpha': 0.011464965108538014, 'solver': 'auto', 'fit_intercept': True}. Best is trial 16 with value: 10.754778731282741.\n",
      "[I 2025-02-16 15:44:14,206] Trial 17 finished with value: 11.799601082875148 and parameters: {'alpha': 0.014650947442853064, 'solver': 'lsqr', 'fit_intercept': True}. Best is trial 16 with value: 10.754778731282741.\n",
      "c:\\Users\\wlyle\\Desktop\\DataScienceDataChallenge\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-02-16 15:44:15,092] Trial 18 finished with value: 12.481907762433433 and parameters: {'alpha': 0.012159105383055596, 'solver': 'sag', 'fit_intercept': True}. Best is trial 16 with value: 10.754778731282741.\n",
      "[I 2025-02-16 15:44:15,102] Trial 19 finished with value: 10.754775081589832 and parameters: {'alpha': 0.021503404780487588, 'solver': 'svd', 'fit_intercept': True}. Best is trial 19 with value: 10.754775081589832.\n",
      "[I 2025-02-16 15:44:15,114] Trial 20 finished with value: 10.7547476491254 and parameters: {'alpha': 0.0972427392184436, 'solver': 'svd', 'fit_intercept': True}. Best is trial 20 with value: 10.7547476491254.\n",
      "[I 2025-02-16 15:44:15,126] Trial 21 finished with value: 10.754747299268141 and parameters: {'alpha': 0.09821194946332097, 'solver': 'svd', 'fit_intercept': True}. Best is trial 21 with value: 10.754747299268141.\n",
      "[I 2025-02-16 15:44:15,137] Trial 22 finished with value: 10.754747559208342 and parameters: {'alpha': 0.09749182872767552, 'solver': 'svd', 'fit_intercept': True}. Best is trial 21 with value: 10.754747299268141.\n",
      "[I 2025-02-16 15:44:15,148] Trial 23 finished with value: 10.75474838133347 and parameters: {'alpha': 0.09521456941209296, 'solver': 'svd', 'fit_intercept': True}. Best is trial 21 with value: 10.754747299268141.\n",
      "[I 2025-02-16 15:44:15,160] Trial 24 finished with value: 10.754750043123698 and parameters: {'alpha': 0.09061285961659786, 'solver': 'svd', 'fit_intercept': True}. Best is trial 21 with value: 10.754747299268141.\n",
      "[I 2025-02-16 15:44:15,171] Trial 25 finished with value: 10.754769495897817 and parameters: {'alpha': 0.03688409938965327, 'solver': 'svd', 'fit_intercept': True}. Best is trial 21 with value: 10.754747299268141.\n",
      "[I 2025-02-16 15:44:15,182] Trial 26 finished with value: 10.84622207739956 and parameters: {'alpha': 0.07737250836747213, 'solver': 'svd', 'fit_intercept': False}. Best is trial 21 with value: 10.754747299268141.\n",
      "[I 2025-02-16 15:44:15,193] Trial 27 finished with value: 10.754770294331516 and parameters: {'alpha': 0.03468425833047783, 'solver': 'svd', 'fit_intercept': True}. Best is trial 21 with value: 10.754747299268141.\n",
      "[I 2025-02-16 15:44:15,200] Trial 28 finished with value: 10.754757105846993 and parameters: {'alpha': 0.07107606763504189, 'solver': 'cholesky', 'fit_intercept': True}. Best is trial 21 with value: 10.754747299268141.\n",
      "[I 2025-02-16 15:44:15,210] Trial 29 finished with value: 10.867749015296955 and parameters: {'alpha': 0.09827918565937106, 'solver': 'svd', 'fit_intercept': False}. Best is trial 21 with value: 10.754747299268141.\n",
      "[I 2025-02-16 15:44:15,220] A new study created in memory with name: no-name-3e6a25d6-4592-4b85-93df-7f37e489a13c\n",
      "[I 2025-02-16 15:44:15,225] Trial 0 finished with value: 11.139425994598326 and parameters: {'alpha': 0.008235188193077161, 'solver': 'sparse_cg', 'fit_intercept': False}. Best is trial 0 with value: 11.139425994598326.\n",
      "[I 2025-02-16 15:44:15,229] Trial 1 finished with value: 11.766780350677736 and parameters: {'alpha': 0.011210390489368267, 'solver': 'lsqr', 'fit_intercept': True}. Best is trial 0 with value: 11.139425994598326.\n",
      "[I 2025-02-16 15:44:15,233] Trial 2 finished with value: 10.768617055789345 and parameters: {'alpha': 0.01583855812647305, 'solver': 'cholesky', 'fit_intercept': True}. Best is trial 2 with value: 10.768617055789345.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 2/5\n",
      "Model name: ridge\n",
      "MAE: 8.572073281281346\n",
      "MSE: 115.66458947111536\n",
      "RMSE: 10.754747299268141\n",
      "PCC: 0.5251901941505763\n",
      "Spearman R: 0.5521689012378521\n",
      "R2 Score: 0.2750354343071011\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wlyle\\Desktop\\DataScienceDataChallenge\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-02-16 15:44:16,123] Trial 3 finished with value: 12.67636366835567 and parameters: {'alpha': 0.030780208622879224, 'solver': 'sag', 'fit_intercept': False}. Best is trial 2 with value: 10.768617055789345.\n",
      "[I 2025-02-16 15:44:16,132] Trial 4 finished with value: 10.768616746210544 and parameters: {'alpha': 0.01885136948915723, 'solver': 'svd', 'fit_intercept': True}. Best is trial 4 with value: 10.768616746210544.\n",
      "[I 2025-02-16 15:44:16,136] Trial 5 finished with value: 12.092737816706338 and parameters: {'alpha': 0.02351309830980028, 'solver': 'lsqr', 'fit_intercept': False}. Best is trial 4 with value: 10.768616746210544.\n",
      "[I 2025-02-16 15:44:16,142] Trial 6 finished with value: 10.767711187572155 and parameters: {'alpha': 0.05333957135075572, 'solver': 'sparse_cg', 'fit_intercept': True}. Best is trial 6 with value: 10.767711187572155.\n",
      "c:\\Users\\wlyle\\Desktop\\DataScienceDataChallenge\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-02-16 15:44:17,028] Trial 7 finished with value: 12.676363665573186 and parameters: {'alpha': 0.0071634194123066, 'solver': 'sag', 'fit_intercept': False}. Best is trial 6 with value: 10.767711187572155.\n",
      "[I 2025-02-16 15:44:17,032] Trial 8 finished with value: 10.767190439369097 and parameters: {'alpha': 0.0037790067561687462, 'solver': 'cholesky', 'fit_intercept': False}. Best is trial 8 with value: 10.767190439369097.\n",
      "[I 2025-02-16 15:44:17,036] Trial 9 finished with value: 12.09273781635285 and parameters: {'alpha': 0.023362198184314235, 'solver': 'lsqr', 'fit_intercept': False}. Best is trial 8 with value: 10.767190439369097.\n",
      "[I 2025-02-16 15:44:17,041] Trial 10 finished with value: 10.76792380597257 and parameters: {'alpha': 0.0015947237578120168, 'solver': 'cholesky', 'fit_intercept': False}. Best is trial 8 with value: 10.767190439369097.\n",
      "c:\\Users\\wlyle\\Desktop\\DataScienceDataChallenge\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-02-16 15:44:18,397] Trial 11 finished with value: 12.695591821399095 and parameters: {'alpha': 0.09051402416661637, 'solver': 'saga', 'fit_intercept': True}. Best is trial 8 with value: 10.767190439369097.\n",
      "[I 2025-02-16 15:44:18,403] Trial 12 finished with value: 10.7686184019719 and parameters: {'alpha': 0.002745993802110594, 'solver': 'auto', 'fit_intercept': True}. Best is trial 8 with value: 10.767190439369097.\n",
      "[I 2025-02-16 15:44:18,410] Trial 13 finished with value: 10.767714334403976 and parameters: {'alpha': 0.0036618170014984168, 'solver': 'sparse_cg', 'fit_intercept': True}. Best is trial 8 with value: 10.767190439369097.\n",
      "[I 2025-02-16 15:44:18,417] Trial 14 finished with value: 11.139424596211112 and parameters: {'alpha': 0.07784959597047834, 'solver': 'sparse_cg', 'fit_intercept': False}. Best is trial 8 with value: 10.767190439369097.\n",
      "[I 2025-02-16 15:44:18,423] Trial 15 finished with value: 10.76861826205259 and parameters: {'alpha': 0.004106168021042369, 'solver': 'cholesky', 'fit_intercept': True}. Best is trial 8 with value: 10.767190439369097.\n",
      "[I 2025-02-16 15:44:18,434] Trial 16 finished with value: 10.768613896647976 and parameters: {'alpha': 0.04661744068038975, 'solver': 'svd', 'fit_intercept': True}. Best is trial 8 with value: 10.767190439369097.\n",
      "c:\\Users\\wlyle\\Desktop\\DataScienceDataChallenge\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-02-16 15:44:19,455] Trial 17 finished with value: 12.717908966497605 and parameters: {'alpha': 0.00143860246817674, 'solver': 'saga', 'fit_intercept': False}. Best is trial 8 with value: 10.767190439369097.\n",
      "[I 2025-02-16 15:44:19,461] Trial 18 finished with value: 10.76861353129868 and parameters: {'alpha': 0.05018187155480754, 'solver': 'auto', 'fit_intercept': True}. Best is trial 8 with value: 10.767190439369097.\n",
      "[I 2025-02-16 15:44:19,467] Trial 19 finished with value: 10.766775483155385 and parameters: {'alpha': 0.005496921059109791, 'solver': 'cholesky', 'fit_intercept': False}. Best is trial 19 with value: 10.766775483155385.\n",
      "[I 2025-02-16 15:44:19,474] Trial 20 finished with value: 10.766778415310133 and parameters: {'alpha': 0.005482478735864086, 'solver': 'cholesky', 'fit_intercept': False}. Best is trial 19 with value: 10.766775483155385.\n",
      "[I 2025-02-16 15:44:19,486] Trial 21 finished with value: 10.766628128125959 and parameters: {'alpha': 0.006291352703385896, 'solver': 'cholesky', 'fit_intercept': False}. Best is trial 21 with value: 10.766628128125959.\n",
      "[I 2025-02-16 15:44:19,495] Trial 22 finished with value: 10.766666159642503 and parameters: {'alpha': 0.006071698201278611, 'solver': 'cholesky', 'fit_intercept': False}. Best is trial 21 with value: 10.766628128125959.\n",
      "[I 2025-02-16 15:44:19,501] Trial 23 finished with value: 10.767681554244657 and parameters: {'alpha': 0.002240849633476958, 'solver': 'cholesky', 'fit_intercept': False}. Best is trial 21 with value: 10.766628128125959.\n",
      "[I 2025-02-16 15:44:19,506] Trial 24 finished with value: 10.766375457138887 and parameters: {'alpha': 0.013322387832143515, 'solver': 'cholesky', 'fit_intercept': False}. Best is trial 24 with value: 10.766375457138887.\n",
      "[I 2025-02-16 15:44:19,513] Trial 25 finished with value: 10.76635276042739 and parameters: {'alpha': 0.013030167776230391, 'solver': 'cholesky', 'fit_intercept': False}. Best is trial 25 with value: 10.76635276042739.\n",
      "[I 2025-02-16 15:44:19,518] Trial 26 finished with value: 10.766267285663954 and parameters: {'alpha': 0.01079740396925041, 'solver': 'cholesky', 'fit_intercept': False}. Best is trial 26 with value: 10.766267285663954.\n",
      "[I 2025-02-16 15:44:19,524] Trial 27 finished with value: 10.76626740053479 and parameters: {'alpha': 0.010840169267589475, 'solver': 'cholesky', 'fit_intercept': False}. Best is trial 26 with value: 10.766267285663954.\n",
      "[I 2025-02-16 15:44:19,531] Trial 28 finished with value: 10.766274285225803 and parameters: {'alpha': 0.010096932518958645, 'solver': 'cholesky', 'fit_intercept': False}. Best is trial 26 with value: 10.766267285663954.\n",
      "c:\\Users\\wlyle\\Desktop\\DataScienceDataChallenge\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-02-16 15:44:20,431] Trial 29 finished with value: 12.67636366572118 and parameters: {'alpha': 0.008549257821380517, 'solver': 'sag', 'fit_intercept': False}. Best is trial 26 with value: 10.766267285663954.\n",
      "[I 2025-02-16 15:44:20,437] A new study created in memory with name: no-name-636c5432-9432-4fdd-a07e-20e600f7a8ad\n",
      "[I 2025-02-16 15:44:20,440] Trial 0 finished with value: 10.508363763057034 and parameters: {'alpha': 0.023250125288464072, 'solver': 'cholesky', 'fit_intercept': False}. Best is trial 0 with value: 10.508363763057034.\n",
      "[I 2025-02-16 15:44:20,447] Trial 1 finished with value: 10.480641428643567 and parameters: {'alpha': 0.013612283592963388, 'solver': 'cholesky', 'fit_intercept': True}. Best is trial 1 with value: 10.480641428643567.\n",
      "[I 2025-02-16 15:44:20,452] Trial 2 finished with value: 10.659375595479089 and parameters: {'alpha': 0.09520081136654074, 'solver': 'auto', 'fit_intercept': False}. Best is trial 1 with value: 10.480641428643567.\n",
      "[I 2025-02-16 15:44:20,460] Trial 3 finished with value: 10.48135784664181 and parameters: {'alpha': 0.004680849016139567, 'solver': 'sparse_cg', 'fit_intercept': True}. Best is trial 1 with value: 10.480641428643567.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 3/5\n",
      "Model name: ridge\n",
      "MAE: 8.640715656164911\n",
      "MSE: 115.91251126635788\n",
      "RMSE: 10.766267285663954\n",
      "PCC: 0.538994754679601\n",
      "Spearman R: 0.5713171616473289\n",
      "R2 Score: 0.28930948238516596\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wlyle\\Desktop\\DataScienceDataChallenge\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-02-16 15:44:21,445] Trial 4 finished with value: 12.6156397083005 and parameters: {'alpha': 0.0815033955737289, 'solver': 'saga', 'fit_intercept': True}. Best is trial 1 with value: 10.480641428643567.\n",
      "[I 2025-02-16 15:44:21,448] Trial 5 finished with value: 11.598384557107469 and parameters: {'alpha': 0.0029413944393056357, 'solver': 'lsqr', 'fit_intercept': True}. Best is trial 1 with value: 10.480641428643567.\n",
      "[I 2025-02-16 15:44:21,454] Trial 6 finished with value: 10.481317946474032 and parameters: {'alpha': 0.026462724399111178, 'solver': 'sparse_cg', 'fit_intercept': True}. Best is trial 1 with value: 10.480641428643567.\n",
      "[I 2025-02-16 15:44:21,464] Trial 7 finished with value: 10.561319242693655 and parameters: {'alpha': 0.04699448392042688, 'solver': 'svd', 'fit_intercept': False}. Best is trial 1 with value: 10.480641428643567.\n",
      "[I 2025-02-16 15:44:21,468] Trial 8 finished with value: 12.002102448344468 and parameters: {'alpha': 0.010648268816015157, 'solver': 'lsqr', 'fit_intercept': False}. Best is trial 1 with value: 10.480641428643567.\n",
      "[I 2025-02-16 15:44:21,472] Trial 9 finished with value: 10.484823432692092 and parameters: {'alpha': 0.00905989448216886, 'solver': 'auto', 'fit_intercept': False}. Best is trial 1 with value: 10.480641428643567.\n",
      "[I 2025-02-16 15:44:21,477] Trial 10 finished with value: 10.480653299287242 and parameters: {'alpha': 0.0013759101759563844, 'solver': 'cholesky', 'fit_intercept': True}. Best is trial 1 with value: 10.480641428643567.\n",
      "[I 2025-02-16 15:44:21,485] Trial 11 finished with value: 10.480653620996874 and parameters: {'alpha': 0.0010451237009030015, 'solver': 'cholesky', 'fit_intercept': True}. Best is trial 1 with value: 10.480641428643567.\n",
      "c:\\Users\\wlyle\\Desktop\\DataScienceDataChallenge\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-02-16 15:44:22,391] Trial 12 finished with value: 12.542398068961434 and parameters: {'alpha': 0.001361511234143895, 'solver': 'sag', 'fit_intercept': True}. Best is trial 1 with value: 10.480641428643567.\n",
      "[I 2025-02-16 15:44:22,396] Trial 13 finished with value: 10.480644928817167 and parameters: {'alpha': 0.00999801932739994, 'solver': 'cholesky', 'fit_intercept': True}. Best is trial 1 with value: 10.480641428643567.\n",
      "[I 2025-02-16 15:44:22,403] Trial 14 finished with value: 10.480643548465137 and parameters: {'alpha': 0.011422738791247958, 'solver': 'cholesky', 'fit_intercept': True}. Best is trial 1 with value: 10.480641428643567.\n",
      "[I 2025-02-16 15:44:22,409] Trial 15 finished with value: 10.480634651920091 and parameters: {'alpha': 0.020624839721428676, 'solver': 'cholesky', 'fit_intercept': True}. Best is trial 15 with value: 10.480634651920091.\n",
      "c:\\Users\\wlyle\\Desktop\\DataScienceDataChallenge\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-02-16 15:44:23,309] Trial 16 finished with value: 12.542398076193011 and parameters: {'alpha': 0.023385413141742625, 'solver': 'sag', 'fit_intercept': True}. Best is trial 15 with value: 10.480634651920091.\n",
      "[I 2025-02-16 15:44:23,319] Trial 17 finished with value: 10.480649313550979 and parameters: {'alpha': 0.005477747725754095, 'solver': 'svd', 'fit_intercept': True}. Best is trial 15 with value: 10.480634651920091.\n",
      "c:\\Users\\wlyle\\Desktop\\DataScienceDataChallenge\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-02-16 15:44:24,302] Trial 18 finished with value: 12.615639704491551 and parameters: {'alpha': 0.03833330483641024, 'solver': 'saga', 'fit_intercept': True}. Best is trial 15 with value: 10.480634651920091.\n",
      "[I 2025-02-16 15:44:24,308] Trial 19 finished with value: 10.480638760369764 and parameters: {'alpha': 0.0163710584545596, 'solver': 'cholesky', 'fit_intercept': True}. Best is trial 15 with value: 10.480634651920091.\n",
      "[I 2025-02-16 15:44:24,314] Trial 20 finished with value: 10.480638028402188 and parameters: {'alpha': 0.01712838687112117, 'solver': 'cholesky', 'fit_intercept': True}. Best is trial 15 with value: 10.480634651920091.\n",
      "[I 2025-02-16 15:44:24,320] Trial 21 finished with value: 10.480638189957494 and parameters: {'alpha': 0.016961214275341606, 'solver': 'cholesky', 'fit_intercept': True}. Best is trial 15 with value: 10.480634651920091.\n",
      "[I 2025-02-16 15:44:24,326] Trial 22 finished with value: 10.480612920478244 and parameters: {'alpha': 0.04324668267779857, 'solver': 'cholesky', 'fit_intercept': True}. Best is trial 22 with value: 10.480612920478244.\n",
      "[I 2025-02-16 15:44:24,332] Trial 23 finished with value: 10.480605826367148 and parameters: {'alpha': 0.05067619557738744, 'solver': 'cholesky', 'fit_intercept': True}. Best is trial 23 with value: 10.480605826367148.\n",
      "[I 2025-02-16 15:44:24,338] Trial 24 finished with value: 10.480602958688943 and parameters: {'alpha': 0.053685761752878224, 'solver': 'cholesky', 'fit_intercept': True}. Best is trial 24 with value: 10.480602958688943.\n",
      "[I 2025-02-16 15:44:24,344] Trial 25 finished with value: 10.480599842844658 and parameters: {'alpha': 0.05695990491349559, 'solver': 'cholesky', 'fit_intercept': True}. Best is trial 25 with value: 10.480599842844658.\n",
      "[I 2025-02-16 15:44:24,354] Trial 26 finished with value: 10.599976580535618 and parameters: {'alpha': 0.06451796619639791, 'solver': 'svd', 'fit_intercept': False}. Best is trial 25 with value: 10.480599842844658.\n",
      "[I 2025-02-16 15:44:24,361] Trial 27 finished with value: 11.598385191506233 and parameters: {'alpha': 0.05848055310795859, 'solver': 'lsqr', 'fit_intercept': True}. Best is trial 25 with value: 10.480599842844658.\n",
      "[I 2025-02-16 15:44:24,367] Trial 28 finished with value: 10.480622116416512 and parameters: {'alpha': 0.033648860870054066, 'solver': 'auto', 'fit_intercept': True}. Best is trial 25 with value: 10.480599842844658.\n",
      "[I 2025-02-16 15:44:24,373] Trial 29 finished with value: 10.5999624595253 and parameters: {'alpha': 0.06451130730091861, 'solver': 'cholesky', 'fit_intercept': False}. Best is trial 25 with value: 10.480599842844658.\n",
      "[I 2025-02-16 15:44:24,379] A new study created in memory with name: no-name-5af8a670-fd69-4a4d-81d4-b755def51652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 4/5\n",
      "Model name: ridge\n",
      "MAE: 8.275275207338948\n",
      "MSE: 109.84297306583547\n",
      "RMSE: 10.480599842844658\n",
      "PCC: 0.5648406527524452\n",
      "Spearman R: 0.58996920311297\n",
      "R2 Score: 0.31694618854879775\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wlyle\\Desktop\\DataScienceDataChallenge\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-02-16 15:44:25,373] Trial 0 finished with value: 12.723142421262807 and parameters: {'alpha': 0.013603112503737629, 'solver': 'saga', 'fit_intercept': False}. Best is trial 0 with value: 12.723142421262807.\n",
      "c:\\Users\\wlyle\\Desktop\\DataScienceDataChallenge\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-02-16 15:44:26,358] Trial 1 finished with value: 12.70161223716092 and parameters: {'alpha': 0.01437977428783033, 'solver': 'saga', 'fit_intercept': True}. Best is trial 1 with value: 12.70161223716092.\n",
      "c:\\Users\\wlyle\\Desktop\\DataScienceDataChallenge\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-02-16 15:44:27,344] Trial 2 finished with value: 12.723142422705655 and parameters: {'alpha': 0.05540823847104073, 'solver': 'saga', 'fit_intercept': False}. Best is trial 1 with value: 12.70161223716092.\n",
      "[I 2025-02-16 15:44:27,349] Trial 3 finished with value: 12.089140797019855 and parameters: {'alpha': 0.013796540684659997, 'solver': 'lsqr', 'fit_intercept': False}. Best is trial 3 with value: 12.089140797019855.\n",
      "[I 2025-02-16 15:44:27,352] Trial 4 finished with value: 10.823797646137415 and parameters: {'alpha': 0.005285603502153508, 'solver': 'cholesky', 'fit_intercept': False}. Best is trial 4 with value: 10.823797646137415.\n",
      "[I 2025-02-16 15:44:27,356] Trial 5 finished with value: 11.062870899418181 and parameters: {'alpha': 0.06619089943718108, 'solver': 'auto', 'fit_intercept': False}. Best is trial 4 with value: 10.823797646137415.\n",
      "[I 2025-02-16 15:44:27,363] Trial 6 finished with value: 10.80019783551174 and parameters: {'alpha': 0.010511919325913716, 'solver': 'sparse_cg', 'fit_intercept': True}. Best is trial 6 with value: 10.80019783551174.\n",
      "c:\\Users\\wlyle\\Desktop\\DataScienceDataChallenge\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-02-16 15:44:28,357] Trial 7 finished with value: 12.701612235971004 and parameters: {'alpha': 0.0011174178551796796, 'solver': 'saga', 'fit_intercept': True}. Best is trial 6 with value: 10.80019783551174.\n",
      "c:\\Users\\wlyle\\Desktop\\DataScienceDataChallenge\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-02-16 15:44:29,253] Trial 8 finished with value: 12.626655123330881 and parameters: {'alpha': 0.0012704872578852779, 'solver': 'sag', 'fit_intercept': True}. Best is trial 6 with value: 10.80019783551174.\n",
      "[I 2025-02-16 15:44:29,258] Trial 9 finished with value: 12.08914076462754 and parameters: {'alpha': 0.006098959152603778, 'solver': 'lsqr', 'fit_intercept': False}. Best is trial 6 with value: 10.80019783551174.\n",
      "[I 2025-02-16 15:44:29,264] Trial 10 finished with value: 10.80019880886297 and parameters: {'alpha': 0.0030686182875523083, 'solver': 'sparse_cg', 'fit_intercept': True}. Best is trial 6 with value: 10.80019783551174.\n",
      "[I 2025-02-16 15:44:29,271] Trial 11 finished with value: 10.800199412514898 and parameters: {'alpha': 0.0035329918930110393, 'solver': 'sparse_cg', 'fit_intercept': True}. Best is trial 6 with value: 10.80019783551174.\n",
      "[I 2025-02-16 15:44:29,279] Trial 12 finished with value: 10.800195756091577 and parameters: {'alpha': 0.02970476718410083, 'solver': 'sparse_cg', 'fit_intercept': True}. Best is trial 12 with value: 10.800195756091577.\n",
      "[I 2025-02-16 15:44:29,290] Trial 13 finished with value: 10.904134679059453 and parameters: {'alpha': 0.02956093023136154, 'solver': 'svd', 'fit_intercept': True}. Best is trial 12 with value: 10.800195756091577.\n",
      "[I 2025-02-16 15:44:29,298] Trial 14 finished with value: 10.800195499112647 and parameters: {'alpha': 0.03005469227325177, 'solver': 'sparse_cg', 'fit_intercept': True}. Best is trial 14 with value: 10.800195499112647.\n",
      "[I 2025-02-16 15:44:29,305] Trial 15 finished with value: 10.800195136045726 and parameters: {'alpha': 0.03186408741238553, 'solver': 'sparse_cg', 'fit_intercept': True}. Best is trial 15 with value: 10.800195136045726.\n",
      "[I 2025-02-16 15:44:29,313] Trial 16 finished with value: 10.800195955676049 and parameters: {'alpha': 0.029125198823670106, 'solver': 'sparse_cg', 'fit_intercept': True}. Best is trial 15 with value: 10.800195136045726.\n",
      "[I 2025-02-16 15:44:29,324] Trial 17 finished with value: 10.900937524324785 and parameters: {'alpha': 0.08368023959223818, 'solver': 'svd', 'fit_intercept': True}. Best is trial 15 with value: 10.800195136045726.\n",
      "[I 2025-02-16 15:44:29,332] Trial 18 finished with value: 10.904578629328427 and parameters: {'alpha': 0.022248164368467285, 'solver': 'auto', 'fit_intercept': True}. Best is trial 15 with value: 10.800195136045726.\n",
      "c:\\Users\\wlyle\\Desktop\\DataScienceDataChallenge\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-02-16 15:44:30,240] Trial 19 finished with value: 12.626655138787484 and parameters: {'alpha': 0.04711019702047524, 'solver': 'sag', 'fit_intercept': True}. Best is trial 15 with value: 10.800195136045726.\n",
      "[I 2025-02-16 15:44:30,247] Trial 20 finished with value: 10.903315363439201 and parameters: {'alpha': 0.04318361797724635, 'solver': 'cholesky', 'fit_intercept': True}. Best is trial 15 with value: 10.800195136045726.\n",
      "[I 2025-02-16 15:44:30,254] Trial 21 finished with value: 10.800196172806002 and parameters: {'alpha': 0.02426373339664991, 'solver': 'sparse_cg', 'fit_intercept': True}. Best is trial 15 with value: 10.800195136045726.\n",
      "[I 2025-02-16 15:44:30,262] Trial 22 finished with value: 10.800194229513453 and parameters: {'alpha': 0.03697422129480843, 'solver': 'sparse_cg', 'fit_intercept': True}. Best is trial 22 with value: 10.800194229513453.\n",
      "[I 2025-02-16 15:44:30,269] Trial 23 finished with value: 10.80019667568207 and parameters: {'alpha': 0.019697660850503795, 'solver': 'sparse_cg', 'fit_intercept': True}. Best is trial 22 with value: 10.800194229513453.\n",
      "[I 2025-02-16 15:44:30,277] Trial 24 finished with value: 10.80018668799134 and parameters: {'alpha': 0.09193761269979547, 'solver': 'sparse_cg', 'fit_intercept': True}. Best is trial 24 with value: 10.80018668799134.\n",
      "[I 2025-02-16 15:44:30,284] Trial 25 finished with value: 10.800186714213462 and parameters: {'alpha': 0.0969075402965892, 'solver': 'sparse_cg', 'fit_intercept': True}. Best is trial 24 with value: 10.80018668799134.\n",
      "[I 2025-02-16 15:44:30,292] Trial 26 finished with value: 10.800185267794662 and parameters: {'alpha': 0.09948879453905773, 'solver': 'sparse_cg', 'fit_intercept': True}. Best is trial 26 with value: 10.800185267794662.\n",
      "[I 2025-02-16 15:44:30,299] Trial 27 finished with value: 10.800186522366804 and parameters: {'alpha': 0.09683174597365335, 'solver': 'sparse_cg', 'fit_intercept': True}. Best is trial 26 with value: 10.800185267794662.\n",
      "[I 2025-02-16 15:44:30,305] Trial 28 finished with value: 10.901548967546416 and parameters: {'alpha': 0.0731274348608766, 'solver': 'auto', 'fit_intercept': True}. Best is trial 26 with value: 10.800185267794662.\n",
      "[I 2025-02-16 15:44:30,312] Trial 29 finished with value: 11.179244446995643 and parameters: {'alpha': 0.09631395743371657, 'solver': 'cholesky', 'fit_intercept': False}. Best is trial 26 with value: 10.800185267794662.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 5/5\n",
      "Model name: ridge\n",
      "MAE: 8.58120062670553\n",
      "MSE: 116.64400181868884\n",
      "RMSE: 10.800185267794662\n",
      "PCC: 0.5355513509482475\n",
      "Spearman R: 0.556710000667969\n",
      "R2 Score: 0.2861560820450474\n",
      "\n",
      "{'ridge': {'mae': 8.569055497148788, 'mse': 115.43710686704317, 'rmse': 10.743215488281184, 'pcc': 0.5421727669051197, 'spearman_r': 0.5679495935349975, 'r2_score': 0.29308239278958387}}\n"
     ]
    }
   ],
   "source": [
    "# Train + Validate models\n",
    "metrics = [\"mae\", \"mse\", \"rmse\", \"pcc\", \"spearman_r\", \"r2_score\"]\n",
    "model_scores = {model_name: defaultdict(list) for model_name in models.keys()}\n",
    "\n",
    "for fold in range(NUM_FOLDS):\n",
    "    fold_data = kfold_data[fold]\n",
    "     \n",
    "    # Extract data\n",
    "    train_data = fold_data[\"train\"]\n",
    "    val_data = fold_data[\"val\"]\n",
    "    test_data = kfold_data[fold][\"test\"]\n",
    "\n",
    "    train_y = train_data[\"outcome\"]\n",
    "    val_y = val_data[\"outcome\"]\n",
    "    test_y = test_data[\"outcome\"]\n",
    "\n",
    "    train_x = train_data.drop(columns=[\"outcome\"])\n",
    "    val_x = val_data.drop(columns=[\"outcome\"])\n",
    "    test_x = test_data.drop(columns=[\"outcome\"])\n",
    "\n",
    "    # print(f\"Fold {fold+1}/{NUM_FOLDS}\")\n",
    "    # print(f\"Train data shape: {train_x.shape} | Train target shape: {train_y.shape}\")\n",
    "    # print(f\"Val data shape: {val_x.shape} | Val target shape: {val_y.shape}\")\n",
    "    # print(f\"Test data shape: {test_x.shape} | Test target shape: {test_y.shape}\")\n",
    "\n",
    "    # Train model\n",
    "    for model_name, model in models.items():\n",
    "        study = optuna.create_study(direction=\"minimize\")\n",
    "        study.optimize(lambda trial: objective(trial=trial, \n",
    "                                               model_type=model, \n",
    "                                               x_train=train_x, \n",
    "                                               y_train=train_y, \n",
    "                                               x_val=val_x, \n",
    "                                               y_val=val_y\n",
    "                                               ), n_trials=30)\n",
    "        \n",
    "        # Train model with best hyperparameters\n",
    "        best_fold_params = study.best_params\n",
    "        model = model(**best_fold_params)\n",
    "        model.fit(train_x, train_y)\n",
    "        preds = model.predict(val_x)\n",
    "\n",
    "        metrics = calculate_metrics(targets=val_y, preds=preds)\n",
    "        mae = metrics[\"mae\"]\n",
    "        mse = metrics[\"mse\"]\n",
    "        rmse = metrics[\"rmse\"]\n",
    "        pcc = metrics[\"pcc\"]\n",
    "        spearman_r = metrics[\"spearman_r\"]\n",
    "        r2_score = metrics[\"r2_score\"]\n",
    "\n",
    "        for metric in metrics:\n",
    "            model_scores[model_name][metric].append(metrics[metric])\n",
    "\n",
    "        print(f\"Fold: {fold+1}/{NUM_FOLDS}\")\n",
    "        print(f\"Model name: {model_name}\")\n",
    "        print(f\"MAE: {mae}\")\n",
    "        print(f\"MSE: {mse}\")\n",
    "        print(f\"RMSE: {rmse}\")\n",
    "        print(f\"PCC: {pcc}\")\n",
    "        print(f\"Spearman R: {spearman_r}\")\n",
    "        print(f\"R2 Score: {r2_score}\")\n",
    "        print()\n",
    "\n",
    "# Compute average scores\n",
    "for model_name, model_metrics in model_scores.items():\n",
    "    for metric, scores in model_metrics.items():\n",
    "        model_scores[model_name][metric] = sum(scores) / len(scores)\n",
    "    model_scores[model_name] = dict(model_scores[model_name])\n",
    "print(model_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
